{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD1 dataset\n",
    "train_dataset = load_dataset(\"squad\")[\"train\"]\n",
    "test_dataset = load_dataset(\"squad\")[\"validation\"]\n",
    "train_dataset=train_dataset.select(range(100))\n",
    "test_dataset=test_dataset.select(range(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        context = example['context']\n",
    "        question = example['question']\n",
    "        answer = example['answers']['text'][0]\n",
    "        \n",
    "        # do encoding of the context and question \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            question,\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding='max_length',   \n",
    "            max_length=384,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        # get start and end positions of answer in input_ids\n",
    "        input_ids = encoding['input_ids']\n",
    "        answer_start = example['answers']['answer_start'][0]\n",
    "        answer_end = answer_start + len(answer)\n",
    "        \n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        for i, token_id in enumerate(input_ids):\n",
    "            if i == answer_start:\n",
    "                start_positions.append(i)\n",
    "            else:\n",
    "                start_positions.append(-100)\n",
    "            \n",
    "            if i == answer_end:\n",
    "                end_positions.append(i)\n",
    "            else:\n",
    "                end_positions.append(-100)\n",
    "        \n",
    "        # Create input tensors\n",
    "        inputs = {\n",
    "            'input_ids': torch.tensor(encoding['input_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(encoding['token_type_ids'], dtype=torch.long),\n",
    "            'start_positions': torch.tensor(start_positions, dtype=torch.float),  # start and end positions should be float\n",
    "            'end_positions': torch.tensor(end_positions, dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        return inputs, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "x=SquadDataset(train_dataset,tokenizer)\n",
    "\n",
    "\n",
    "# Create the Dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    SquadDataset(train_dataset, tokenizer),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SquadDataset(test_dataset, tokenizer),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    \n",
    "    print(f\"Our context is:\\n {batch[0]['input_ids']}\")\n",
    "#     print(f\"Our context is:\\n {batch['context'][0]}\")\n",
    "#     print(f\"Question: {batch['question'][0]}\")\n",
    "#     print(f\"Answer: {batch['answer'][0]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForQuestionAnswering\n",
    "\n",
    "model = GPT2ForQuestionAnswering.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "epochs = 3\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer):\n",
    "    \n",
    "    # set the model to training model\n",
    "    model.train()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # previous tokens\n",
    "        input_ids = batch[0]['input_ids'].to(device)\n",
    "        attention_mask = batch[0]['attention_mask'].to(device)\n",
    "        token_type_ids = batch[0]['token_type_ids'].to(device)\n",
    "        start_positions = batch[0]['start_positions'].to(device)\n",
    "        end_positions = batch[0]['end_positions'].to(device)\n",
    "        \n",
    "        labels = {\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n",
    "        \n",
    "       # get outputs from model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_start = nn.CrossEntropyLoss()(outputs.start_logits, start_positions)\n",
    "        loss_end = nn.CrossEntropyLoss()(outputs.end_logits, end_positions)\n",
    "        loss = (loss_start + loss_end) / 2  # average loss for start and end positions\n",
    "        \n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    # set the model of evaluation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # previous tokens\n",
    "            input_ids = batch[0]['input_ids'].to(device)\n",
    "            attention_mask = batch[0]['attention_mask'].to(device)\n",
    "            token_type_ids = batch[0]['token_type_ids'].to(device)\n",
    "            start_positions = batch[0]['start_positions'].to(device)\n",
    "            end_positions = batch[0]['end_positions'].to(device)\n",
    "            answer = batch[0]['answer'].to(device)\n",
    "\n",
    "            labels = {\n",
    "                'start_positions': start_positions,\n",
    "                'end_positions': end_positions\n",
    "            }\n",
    "\n",
    "           # get outputs from model\n",
    "            inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            #outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            answer_start_index = outputs.start_logits.argmax()\n",
    "            answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "            predict_answer_tokens = input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "\n",
    "            # target is \"nice puppet\"\n",
    "            target_start_index = torch.tensor([14])\n",
    "            target_end_index = torch.tensor([15])\n",
    "\n",
    "            outputs = model(input_ids, start_positions=target_start_index, end_positions=target_end_index)\n",
    "            loss = outputs.loss\n",
    "            # calculate loss\n",
    "            '''loss_start = nn.CrossEntropyLoss()(outputs.start_logits, start_positions)\n",
    "            loss_end = nn.CrossEntropyLoss()(outputs.end_logits, end_positions)\n",
    "            loss = (loss_start + loss_end) / 2  # average loss for start and end positions\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Print the validation loss for this epoch\n",
    "    print(f\"Validation Loss: {val_loss/len(dataloader)}\")'''\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "for t in tqdm(range(epochs)):\n",
    "    print(f\"Epoch {t+1}\\n ---------------------------\")\n",
    "    train_loop(train_dataloader, model, optimizer)\n",
    "    \n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained('fine_tuned_gpt_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
